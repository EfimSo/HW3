{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWjR-pgsd-Si"
   },
   "source": [
    "# CS640 Homework 3: Neural Networks\n",
    "\n",
    "In this assignment, you will\n",
    "\n",
    "1. derive both forward and backward propagation,\n",
    "2. implement a neural network from scratch, and\n",
    "3. run experiments with your model.\n",
    "\n",
    "### Collaboration\n",
    "You are allowed to work in a team of at most **three** on the coding part(**Q2**), but you must run the experiments and answer written questions independently.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "### General Instructions\n",
    "In an ipython notebook, to run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctrl+Enter` or `[>|]`(like \"play\") button above. To edit any code or text cell (double) click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above.\n",
    "\n",
    "Most of the written questions are followed up a cell for you enter your answers. Please enter your answers in a new line below the **Answer** mark. If you do not see such cell, please insert one by yourself. Your answers and the questions should **not** be in the same cell.\n",
    "\n",
    "### Instructions on Math\n",
    "Some questions require you to enter math expressions. To enter your solutions, put down your derivations into the corresponding cells below using LaTeX. Show all steps when proving statements. If you are not familiar with LaTeX, you should look at some tutorials and at the examples listed below between \\$..\\$. The [OEIS website](https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols) can also be helpful.\n",
    "\n",
    "Alternatively, you can scan your work from paper and insert the image(s) in a text cell.\n",
    "\n",
    "## Submission\n",
    "Once you are ready, save the note book as PDF file (File -> Print -> Save as PDF) and submit via Gradescope. In case some contents cannot be displayed properly in the PDF file, check out this tool: [Colab2PDF](https://github.com/drengskapur/colab2pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9ImiOu7jzNu"
   },
   "source": [
    "## Q0: Name(s)\n",
    "\n",
    "Please write your name in the next cell. If you are collaborating with someone, please list their names as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4ldlvqYkyKw"
   },
   "source": [
    "**Efim Sokolov**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NkWp9N_jDa0"
   },
   "source": [
    "## Q1: Written Problems\n",
    "\n",
    "Consider a simple neural network with three layers: an input layer, a hidden layer, and an output layer.\n",
    "\n",
    "Let $w^{(1)}$ and $w^{(2)}$ be the layers' weight matrices and let $b^{(1)}$ and $b^{(2)}$ be their biases. For convention, suppose that $w_{ij}$ is the weight between the $i$th node in the previous layer and the $j$th node in the current one.\n",
    "\n",
    "Additionally, the activation function for both layers is the sigmoid function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. Let $z^{(1)}$ and $z^{(2)}$ be the outputs of the two layers before activation, and let $a^{(1)} = \\sigma(z^{(1)})$ and $a^{(2)} = \\sigma(z^{(2)})$.\n",
    "\n",
    "Lastly, we choose the L2 loss $L(y_{\\text{true}}, y_{\\text{predict}}) = \\frac{1}{2}(y_{\\text{true}} - y_{\\text{predict}})^{2}$ as the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swVLLpgYmtxv"
   },
   "source": [
    "### Q1.1: Forward Pass\n",
    "Suppose that\n",
    "\n",
    "$w^{(1)} = \\begin{bmatrix}0.4 & 0.6 & 0.2 \\\\ 0.3 & 0.9 & 0.5\\end{bmatrix}$,\n",
    " $b^{(1)} = [1, 1, 1]$; and\n",
    "\n",
    "$w^{(2)} = \\begin{bmatrix}0.2 \\\\ 0.2 \\\\ 0.8\\end{bmatrix}$, $b^{(2)} = [0.5]$.\n",
    "\n",
    "If the input is $a^{(0)} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$, what is the network output? Show your calculation steps and round your **final** answer to 2 digits after decimal.\n",
    "\n",
    "**Note**: You should NOT round any intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmZ7isA1s9rT"
   },
   "source": [
    "$  z^{(1)} = {w^{(1)}}^\\top a^{(0)} + b^{(1)} =  \\begin{bmatrix}  0.7 \\\\  1.5 \\\\ 0.7 \\end{bmatrix} + \\begin{bmatrix}  1 \\\\  1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix}  1.7 \\\\  2.5 \\\\ 1.7 \\end{bmatrix} \\\\[1 em] $\n",
    "$a^{(1)} = \\sigma(z^{(1)} ) = \\begin{bmatrix}  0.8455347349164652 \\\\ 0.9241418199787566 \\\\ 0.8455347349164652 \\end{bmatrix} \\\\[1 em]$\n",
    "$ z^{(2)} = {w^{(2)}}^\\top a^{(1)} + b^{(2)} = 1.0303630989122166 + 0.5 = 1.5303630989122166 \\\\[1 em]$\n",
    "$ a^{(2)} = \\sigma(z^{(2)}) = 0.822059433706368 \\\\[1 em]$\n",
    "$ a^{(2)} \\approx 0.82$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw_yNYW4t2Z0"
   },
   "source": [
    "### Q1.2: Backward Propagation\n",
    "\n",
    "Derive the expressions of the following gradients:\n",
    "1. $\\frac{\\partial L}{\\partial w^{(2)}}$ and $\\frac{\\partial L}{\\partial b^{(2)}}$\n",
    "2. $\\frac{\\partial L}{\\partial w^{(1)}}$ and $\\frac{\\partial L}{\\partial b^{(1)}}$\n",
    "\n",
    "For each gradient, start by deriving the element-level expression using chain rule, and then construct the final answer in matrix form. You can use a self-defined variable(e.g., $\\eta$) to shorten a long expression (especially for the first-layer gradients).\n",
    "\n",
    "An example of your answer should look like the following.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "(Element level)\n",
    "\\begin{align}\n",
    "    \\frac{\\partial L}{\\partial w^{(2)}_{i}} &= \\frac{\\partial L}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial w^{(2)}_{i}} \\text{  (chain rule)}\\\\\n",
    "    &= L'(y_{\\text{true}}, a^{(2)}) \\cdot f'_{2}(z^{(2)}) \\cdot a^{(1)}_{i} \\text{  (substitution)}\\\\\n",
    "    &= ... \\text{  (further substitution and/or simplification if needed)}\n",
    "\\end{align}\n",
    "\n",
    "(Matrix form)\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^{(2)}} = ... \\text{  (only the final answer is needed)}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Note**: The derivative of $\\sigma(x)$ is $\\sigma(x)(1 - \\sigma(x))$ if $x$ is a scalar, or $\\sigma(x)\\odot(1 - \\sigma(x))$ if $x$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9AkrYYVLnRl"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial w^{(2)}_{i}} &= \\frac{\\partial L}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial w^{(2)}_{i}} \\\\\n",
    "    &= L'(y_{\\text{true}}, a^{(2)}) \\cdot f'_{2}(z^{(2)}) \\cdot a^{(1)}_{i} \\\\\n",
    "    &= 2 (a^{(2)} - y) \\cdot \\sigma'(z^{(2)}) \\cdot a^{(1)}_{i} \\\\ \n",
    "    &= 2 (a^{(2)} - y) \\cdot \\sigma(z^{(2)})(1 - \\sigma(z^{(2)})) \\cdot a^{(1)}_{i} \n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w^{(2)}} = a^{(2)} {\\eta^{(3)}}^\\top\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial b^{(2)}_{i}} &= \\frac{\\partial L}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(2)}_{i}} \\\\\n",
    "    &= L'(y_{\\text{true}}, a^{(2)}) \\cdot f'_{2}(z^{(2)}) \\cdot 1 \\\\\n",
    "    &= 2 (a^{(2)} - y) \\cdot \\sigma'(z^{(2)}) \\\\ \n",
    "    &= 2 (a^{(2)} - y) \\cdot \\sigma(z^{(2)})(1 - \\sigma(z^{(2)}))  \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial w^{(2)}_{i}} &= \\frac{\\partial L}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial w^{(2)}_{i}} \\cdot \\frac{\\partial w^{(2)_{i}}}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial w^{(1)}_{i}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial b^{(2)}_{i}} &= \\frac{\\partial L}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(2)}_{i}} \\cdot \\frac{\\partial b^{(2)_{i}}}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial b^{(1)}_{i}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w^{(1)}} = a^{(1)} {\\eta^{(2)}}^\\top\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6AxRuFOlU_B"
   },
   "source": [
    "## Q2: Implementation\n",
    "\n",
    "In this part, you need to construct a neural network model and run a test experiment. We provide a skeleton script of for the model and full script for the test experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce5gxnBCm7HW"
   },
   "source": [
    "### Q2.0: Import Packages\n",
    "\n",
    "The packages imported in the following block should be sufficient for this problem, but you are free to add more if necessary. However, keep in mind that you **should not** import and use any neural network package. If you have concern about an addition package, please contact us via Piazza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rLhHI0rOdQJ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9TiTFf9psFl"
   },
   "source": [
    "### Q2.1: Define Activation and Loss Functions\n",
    "\n",
    "Complete the following functions except for `d_softmax`. The ones starting with a \"d\" are the derivatives of the corresponding functions.\n",
    "\n",
    "Definitions:\n",
    "1. sigmoid: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "2. softmax: softmax(x) $= \\frac{e^{x_{i}}}{\\sum_{i} e^{x_{i}}}$\n",
    "3. L2 loss: $L(y_{\\text{true}}, y_{\\text{predict}}) = \\frac{1}{2}(y_{\\text{true}} - y_{\\text{predict}})^{2}$\n",
    "4. cross entropy loss: $L(y_{\\text{true}}, y_{\\text{predict}}) = -\\sum_{i}y_{\\text{true}}[i]\\cdot\\log y_{\\text{predict}}[i]$\n",
    "\n",
    "**Note**: Although you are free to decide the object types of the input parameters, it is best to assume (and follow this assumption in later sections) that they are either scalars (in rare cases) or Numpy arrays.\n",
    "\n",
    "**Clarification**: The softmax function and the cross entropy loss are almost always used in tandem due to the nice chained derivative they produce. However, their individual derivatives are hard to compute. Therefore, we use a function `d_cross_entropy_softmax` as a loss derivative to house the chained derivative, and a trivial function `d_softmax` as a filler for consistency in implementation. This should be clearer in the network implementation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yuxWGvbhp5jD"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def l2_loss(YTrue, YPredict):\n",
    "    return np.mean((YTrue - YPredict) ** 2)\n",
    "\n",
    "def d_l2_loss(YTrue, YPredict):\n",
    "    return 2 * (YPredict - YTrue)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0, keepdims=True)\n",
    "\n",
    "def d_softmax(x):\n",
    "    return 1.0\n",
    "\n",
    "def cross_entropy_loss(YTrue, YPredict):\n",
    "    return -np.sum(YTrue * np.log(YPredict))\n",
    "\n",
    "def d_cross_entropy_softmax(YTrue, YPredict):\n",
    "    return YPredict - YTrue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvhPU4YqoVZF"
   },
   "source": [
    "### Q2.2: Define the Layer Class\n",
    "\n",
    "The following block defines the Layer class. There is nothing you need to do but run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6r5h0hT8ofqQ"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input, n_output, add_bias = True):\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.add_bias = add_bias\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases with small random values.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(2) # for reproducibility\n",
    "        self.weights = rng.normal(loc = 0, scale = 1, size = (self.n_input, self.n_output))\n",
    "        if self.add_bias:\n",
    "            self.bias = rng.normal(loc = 0, scale = 1, size = (1, self.n_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXcQsl_XpAOW"
   },
   "source": [
    "### Q2.3: Define the Network Class\n",
    "\n",
    "Complete the `fit` and `predict` functions as instructed in the comments. Do not change their input arguments, but you are free to add functions as necessary. The `__init__` function should be left as it is.\n",
    "\n",
    "*Hint \\#1*: This is the heaviest part of this assignment. We recommend you to first go over the math carefully before starting this part.\n",
    "\n",
    "*Hint \\#2*: You are strongly encouraged to use numpy for matrix operations. When doing multiplication, please be extra careful about the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "-p1xdHw0pKr8"
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers, activation_list, d_activation_list, loss_function, d_loss_function):\n",
    "        self.layers = layers\n",
    "        self.activation_list = activation_list\n",
    "        self.d_activation_list = d_activation_list\n",
    "        self.loss_function = loss_function\n",
    "        self.d_loss_function = d_loss_function\n",
    "\n",
    "    def fit(self, X, Y, epochs, learning_rate, reg_lambda):\n",
    "        \"\"\"\n",
    "        This is the training function. It should return the average loss over samples.\n",
    "        \"\"\"\n",
    "        loss = np.zeros(epochs) # stores loss for each epoch\n",
    "        n_samples = len(X)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            current_loss = 0.0 # this should be accumulated over the samples\n",
    "            ######################## start of your code ########################\n",
    "            # first, initialize zero gradients\n",
    "            weight_gradients = [np.zeros_like(layer.weights) for layer in self.layers]\n",
    "            bias_gradients = [np.zeros_like(layer.bias) if layer.add_bias else None for layer in self.layers]\n",
    "\n",
    "             # next, for each sample, do\n",
    "            for sample_index in range(n_samples):\n",
    "                x = X[sample_index]\n",
    "                y = Y[sample_index]\n",
    "                y = Y[sample_index].reshape(1, -1)\n",
    "                activation = x.reshape(1, -1)\n",
    "                activations = [activation]\n",
    "                zs = []\n",
    "\n",
    "                # 1. compute outputs from each layer (via forward propagation);\n",
    "                for layer_index, layer in enumerate(self.layers):\n",
    "                    z = activation @ layer.weights\n",
    "                    if layer.add_bias and bias_gradients[layer_index] is not None:\n",
    "                        z += layer.bias  # Broadcasting\n",
    "                    zs.append(z)\n",
    "                    activation = self.activation_list[layer_index](z)\n",
    "                    activations.append(activation)\n",
    "\n",
    "                # 2. compute and accumulate the current loss over the samples (using self.loss_function); and\n",
    "                current_loss += self.loss_function(activations[-1], y)\n",
    "\n",
    "                # 3. compute and accumulate the gradients (via backward propagation)\n",
    "                # Compute delta at output layer\n",
    "                delta = self.d_loss_function(activations[-1], y) * self.d_activation_list[-1](zs[-1])\n",
    "\n",
    "                # Backward pass\n",
    "                for layer_index in reversed(range(len(self.layers) - 1)): \n",
    "                    activation_prev = activations[layer_index]  # Activation from previous layer\n",
    "                    # Compute gradients\n",
    "                    grad_weights = activation_prev.T @ delta\n",
    "                    weight_gradients[layer_index] += grad_weights\n",
    "                    if self.layers[layer_index].add_bias and bias_gradients[layer_index] is not None:\n",
    "                        grad_biases = delta  # delta is (1, n_output)\n",
    "                        bias_gradients[layer_index] += grad_biases  # Accumulate bias gradients\n",
    "                    if layer_index > 0:\n",
    "                        # Compute delta for previous layer\n",
    "                         delta = (delta @ self.layers[layer_index].weights.T) * self.d_activation_list[layer_index - 1](zs[layer_index - 1]) * activations[layer_index - 1] \n",
    "\n",
    "            # then, update weights and biases using the corresponding mean gradients\n",
    "            # (i.e., accumulated gradient divided by n_samples)\n",
    "            for layer_index, layer in enumerate(self.layers):\n",
    "                # Update weights with L2 regularization\n",
    "                layer.weights -= learning_rate * ((weight_gradients[layer_index] / n_samples) + reg_lambda * layer.weights)\n",
    "                # Update biases\n",
    "                if layer.add_bias and layer.bias is not None:\n",
    "                    layer.bias -= learning_rate * (bias_gradients[layer_index] / n_samples)\n",
    "            ######################## end of your code ##########################\n",
    "            loss[epoch] = current_loss / n_samples\n",
    "        \n",
    "        # lastly, return the average loss\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X, threshold = None):\n",
    "        \"\"\"\n",
    "        This function predicts the labels for samples in X. The parameter threshold\n",
    "        is used when the labels are binary and there is only one node in the final\n",
    "        layer of the network.\n",
    "        \"\"\"\n",
    "        YPredict = np.zeros(len(X))\n",
    "\n",
    "        ######################### start of your code ###########################\n",
    "        # for each sample, run a forward pass and append the predicted label to YPredict\n",
    "        for sample_index in range(len(X)):\n",
    "            x = X[sample_index]\n",
    "            activation = x.reshape(1, -1)\n",
    "            # Forward pass\n",
    "            for layer_index, layer in enumerate(self.layers):\n",
    "                z = activation @ layer.weights\n",
    "                if layer.add_bias:\n",
    "                    z += layer.bias\n",
    "                activation = self.activation_list[layer_index](z)\n",
    "            # Now activation is the output\n",
    "            if threshold is not None and activation.shape[1] == 1:\n",
    "                # Binary classification\n",
    "                YPredict[sample_index] = (activation >= threshold).astype(int).item()\n",
    "            else:\n",
    "                # Multi-class classification\n",
    "                YPredict[sample_index] = np.argmax(activation, axis=1).item()\n",
    "        ######################### end of your code #############################\n",
    "\n",
    "        return YPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NS8f9qz0fSK"
   },
   "source": [
    "### Q2.4: Test Model\n",
    "\n",
    "Use the following example code to test your model with some simple data.\n",
    "\n",
    "**Make sure to produce a decreasing loss curve here before moving on. A failed model implementation will lead to zero point for all remaining problems.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "gTyLlzEa0a0U"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn import datasets\n",
    "\n",
    "X, Y = datasets.load_iris(return_X_y = True)\n",
    "X, Y = X[:100, :2], Y[:100]\n",
    "rng = np.random.default_rng(2)\n",
    "indices = [i for i in range(100)]\n",
    "rng.shuffle(indices)\n",
    "X, Y = X[indices], Y[indices]\n",
    "\n",
    "\n",
    "# assemble your model\n",
    "model = Network([Layer(2, 4), Layer(4, 1)],\n",
    "                [sigmoid, sigmoid],\n",
    "                [d_sigmoid, d_sigmoid],\n",
    "                l2_loss, d_l2_loss)\n",
    "\n",
    "# specify training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 1e-2\n",
    "reg_lambda = 0\n",
    "\n",
    "loss = model.fit(X, Y, epochs, learning_rate, reg_lambda)\n",
    "\n",
    "# plot the losses, the curve should be decreasing\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.plot([i + 1 for i in range(epochs)], loss)\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGasEudB__Dh"
   },
   "source": [
    "## Q3: Real Data Experiments\n",
    "\n",
    "In this part, you need to try out different model parameter values and observe how they affect the results.\n",
    "\n",
    "For each of the questions below, implement experiments and insert performance scores to the designated dictionary. The performance scores can be computed using the imported functions (for F1 score, you need to specify `average = \"macro\"` when calling the function). You can refer to Q2.4 as an example of implementing experiments.\n",
    "\n",
    "**Note**: Remember to initialize a new instance of your model for each different choice of hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA6nhLVxYJdO"
   },
   "source": [
    "### Q3.0: Loading Data\n",
    "\n",
    "Modify the \"data_dir\" variable in the following block and run the cell to load the data. Since the provided dataset contains more than two labels, both \"YTrain\" and \"YTest\" have been converted to one-hot forms.\n",
    "\n",
    "**Note**: Be careful about the shapes of the data variables. Specifically, the one-hot encoded labels are **row vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "H7Pf9GBtMMYK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "XTrain.shape = (898, 64)\n",
      "XTest.shape = (899, 64)\n",
      "YTrain.shape = (898, 1)\n",
      "YTrain_encoded.shape = (898, 10)\n",
      "YTest.shape  (899, 1)\n",
      "YTest_encoded.shape  (899, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = \"dataset\" # input the path to \"dataset\" directory\n",
    "df_X_train = pandas.read_csv(os.path.join(data_dir, \"Digit_X_train.csv\"), header = None)\n",
    "df_X_test = pandas.read_csv(os.path.join(data_dir, \"Digit_X_test.csv\"), header = None)\n",
    "df_y_train = pandas.read_csv(os.path.join(data_dir, \"Digit_y_train.csv\"), header = None)\n",
    "df_y_test = pandas.read_csv(os.path.join(data_dir, \"Digit_y_test.csv\"), header = None)\n",
    "XTrain, XTest = df_X_train.values, df_X_test.values\n",
    "YTrain, YTest = df_y_train.values, df_y_test.values\n",
    "print(\"All labels: \" + str(np.unique(YTrain)))\n",
    "\n",
    "# encode multi-class labels\n",
    "encoder = OneHotEncoder(sparse_output = False)\n",
    "YTrain_encoded = encoder.fit_transform(YTrain)\n",
    "YTest_encoded = encoder.transform(YTest)\n",
    "\n",
    "print(\"XTrain.shape = \" + str(XTrain.shape))\n",
    "print(\"XTest.shape = \" + str(XTest.shape))\n",
    "print(\"YTrain.shape = \" + str(YTrain.shape))\n",
    "print(\"YTrain_encoded.shape = \" + str(YTrain_encoded.shape))\n",
    "print(\"YTest.shape  \" + str(YTest.shape))\n",
    "print(\"YTest_encoded.shape  \" + str(YTest_encoded.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyItn83F-tBV"
   },
   "source": [
    "### Q3.1: Epochs\n",
    "\n",
    "Experiment with **five** different choices of total epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, scores):\n",
    "    TP, FP, FN = 0, 0, 0 \n",
    "    for i, y_hat in enumerate(y_pred):\n",
    "        if y_hat == 1 and y_true[i] == 1:\n",
    "            TP += 1\n",
    "        elif y_hat == 0 and y_true[i] == 1:\n",
    "            FN += 1\n",
    "        elif y_hat == 1 and y_true[i] == 0:\n",
    "            TP += FP\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    scores[\"F1 (Macro)\"].append(F1)\n",
    "    scores[\"Accuracy\"].append(np.mean(y_pred == y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6cKLcwf_K7D"
   },
   "outputs": [],
   "source": [
    "scores_train = {\"Accuracy\" : [], \"F1 (Macro)\" : []}\n",
    "scores_test = {\"Accuracy\" : [], \"F1 (Macro)\" : []}\n",
    "\n",
    "############################## start of your code ##############################\n",
    "epochs = [50, 100, 300, 1000, 3000] # fill the list with your five epoch choices in increasing order\n",
    "\n",
    "# implement experiments and fill the lists in \"scores_train\" and\n",
    "# \"scores_test\" (one entry per epoch value)\n",
    "for epoch in epochs:\n",
    "    model = Network([Layer(64, 32), Layer(32, 10)],\n",
    "                [sigmoid, sigmoid],\n",
    "                [d_sigmoid, d_sigmoid],\n",
    "                l2_loss, d_l2_loss)\n",
    "    loss = model.fit(XTrain, YTrain_encoded, epoch, 1e-3, 1e-3)\n",
    "    Y_pred = model.predict(XTrain)[0]\n",
    "    calculate_metrics(YTrain_encoded, Y_pred, scores_train)\n",
    "\n",
    "    Y_pred = model.predict(XTest)[0]\n",
    "    calculate_metrics(YTest_encoded, Y_pred, scores_test)\n",
    "############################### end of your code ###############################\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 4))\n",
    "for i, key in enumerate([\"Accuracy\", \"F1 (Macro)\"]):\n",
    "    axes[i].plot(epochs, scores_train[key], \"-o\", label = \"train\")\n",
    "    axes[i].plot(epochs, scores_test[key], \"-o\", label = \"test\")\n",
    "    axes[i].set_title(key)\n",
    "    axes[i].set_ylim([0, 1])\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwDcgfbpDHF4"
   },
   "source": [
    "### Q3.2: Learning Rate\n",
    "\n",
    "Experiment with **five** different choices of learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEMZuxy9DRAg"
   },
   "outputs": [],
   "source": [
    "scores_train = {\"Accuracy\" : [], \"F1 (Macro)\" : []}\n",
    "scores_test = {\"Accuracy\" : [], \"F1 (Macro)\" : []}\n",
    "\n",
    "############################## start of your code ##############################\n",
    "LRs = [1e-4, 1e-3, 1e-2, 1e-1, 0.5] # fill the list with your five LR choices in increasing order\n",
    "\n",
    "# implement experiments and fill the lists in \"scores_train\" and\n",
    "# \"scores_test\" (one entry per LR value)\n",
    "x_train, y_train = [], []\n",
    "x_pred, y_pred = [], []\n",
    "\n",
    "def run_experiment():\n",
    "    for lr in LRs:\n",
    "        model = Network([Layer(64, 32), Layer(32, 10)],\n",
    "                    [sigmoid, sigmoid],\n",
    "                    [d_sigmoid, d_sigmoid],\n",
    "                    l2_loss, d_l2_loss)\n",
    "        loss = model.fit(XTrain, YTrain_encoded, 300, lr, 1e-3)\n",
    "        Y_pred = model.predict(XTrain)[0]\n",
    "        calculate_metrics(YTrain_encoded, Y_pred, scores_train)\n",
    "\n",
    "        Y_pred = model.predict(XTest)[0]\n",
    "        calculate_metrics(YTest_encoded, Y_pred, scores_test)\n",
    "        \n",
    "run_experiment()\n",
    "############################### end of your code ###############################\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 4))\n",
    "for i, key in enumerate([\"Accuracy\", \"F1 (Macro)\"]):\n",
    "    axes[i].plot(LRs, scores_train[key], \"-o\", label = \"train\")\n",
    "    axes[i].plot(LRs, scores_test[key], \"-o\", label = \"test\")\n",
    "    axes[i].set_title(key)\n",
    "    axes[i].set_ylim([0, 1])\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96i2ZAsBDTaZ"
   },
   "source": [
    "### Q3.3: Regularization Parameter\n",
    "\n",
    "Experiment with **five** different choices of regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANhxVaLcDfjn"
   },
   "outputs": [],
   "source": [
    "scores_train = {\"Accuracy\" : [], \"F1 (Macro)\" : []}\n",
    "scores_test = {\"Accuracy\" : [], \"F1 (Macro)\" : []}\n",
    "\n",
    "############################## start of your code ##############################\n",
    "lambdas = [0, 0.01, 0.1, 0.5, 1] # fill the list with your five regularization lambda choices in increasing order\n",
    "\n",
    "# implement experiments and fill the lists in \"scores_train\" and\n",
    "# \"scores_test\" (one entry per reg_lambda value)\n",
    "def run_experiment():\n",
    "    for lam in lambdas:\n",
    "        model = Network([Layer(64, 32), Layer(32, 10)],\n",
    "                    [sigmoid, sigmoid],\n",
    "                    [d_sigmoid, d_sigmoid],\n",
    "                    l2_loss, d_l2_loss)\n",
    "        loss = model.fit(XTrain, YTrain_encoded, 300, 1e-3, lam)\n",
    "        Y_pred = model.predict(XTrain)[0]\n",
    "        calculate_metrics(YTrain_encoded, Y_pred, scores_train)\n",
    "\n",
    "        Y_pred = model.predict(XTest)[0]\n",
    "        calculate_metrics(YTest_encoded, Y_pred, scores_test)\n",
    "        \n",
    "run_experiment()\n",
    "############################### end of your code ###############################\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 4))\n",
    "for i, key in enumerate([\"Accuracy\", \"F1 (Macro)\"]):\n",
    "    axes[i].plot(lambdas, scores_train[key], \"-o\", label = \"train\")\n",
    "    axes[i].plot(lambdas, scores_test[key], \"-o\", label = \"test\")\n",
    "    axes[i].set_title(key)\n",
    "    axes[i].set_ylim([0, 1])\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_LjwNPkDgyq"
   },
   "source": [
    "### Q3.4: Network Structure\n",
    "\n",
    "Experiment with **five** different choices of network structure. This includes number of layers and number of nodes in each layer.\n",
    "\n",
    "*Hint*: Try experimenting with increasing complexity. You may need to hard-code the experiments without using a for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zae3B1jqFhHd"
   },
   "outputs": [],
   "source": [
    "levels = [1, 2, 3, 4, 5]\n",
    "scores_train = {\"Accuracy\" : [], \"F1 (Macro)\" : []}\n",
    "scores_test = {\"Accuracy\" : [], \"F1 (Macro)\" : []}\n",
    "architectures = [\n",
    "    [Layer(64, 32), Layer(32, 10)],\n",
    "    [Layer(64, 32), Layer(32, 16), Layer(16, 10)],\n",
    "    [Layer(64, 64), Layer(64, 32), Layer(32, 16), Layer(16, 10)],\n",
    "    [Layer(64, 128), Layer(128, 64), Layer(64, 32), Layer(32, 16), Layer(16, 10)],\n",
    "    [Layer(64, 256), Layer(256, 128), Layer(128, 64), Layer(64, 32), Layer(32, 16), Layer(16, 10)]\n",
    "]\n",
    "############################## start of your code ##############################\n",
    "# implement experiments and fill the lists in \"scores_train\" and\n",
    "# \"scores_test\" (one entry per complexity level)\n",
    "def run_experiment():\n",
    "    for i in range(len(architectures)):\n",
    "        model = Network(architectures[i],\n",
    "                    [sigmoid, sigmoid],\n",
    "                    [d_sigmoid, d_sigmoid],\n",
    "                    l2_loss, d_l2_loss)\n",
    "        loss = model.fit(XTrain, YTrain_encoded, 300, 1e-3, 1e-3)\n",
    "        Y_pred = model.predict(XTrain)[0]\n",
    "        calculate_metrics(YTrain_encoded, Y_pred, scores_train)\n",
    "\n",
    "        Y_pred = model.predict(XTest)[0]\n",
    "        calculate_metrics(YTest_encoded, Y_pred, scores_test)\n",
    "        \n",
    "run_experiment()\n",
    "############################### end of your code ###############################\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 4))\n",
    "for i, key in enumerate([\"Accuracy\", \"F1 (Macro)\"]):\n",
    "    axes[i].plot(levels, scores_train[key], \"-o\", label = \"train\")\n",
    "    axes[i].plot(levels, scores_test[key], \"-o\", label = \"test\")\n",
    "    axes[i].set_title(key)\n",
    "    axes[i].set_ylim([0, 1])\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmRKZaJPFpm1"
   },
   "source": [
    "## Q4: Follow-up Questions\n",
    "\n",
    "For each question below, provide a short answer. You can cite your code if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPFklnbuAwg_"
   },
   "source": [
    "### Q4.1: Briefly describe the workflow of how your model classifies the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8RYC8yiBEvU"
   },
   "source": [
    "The model classifies data by first training a neural network using supervised learning on the provided dataset. During training, it performs forward propagation to compute the outputs for each input sample and then calculates the loss using the cross-entropy loss function. It then conducts backward propagation to update the network's weights and biases based on the gradients of the loss with respect to these parameters. After training, the model predicts the class labels of new, unseen data by feeding the inputs through the network (forward propagation) and using the softmax activation function in the output layer to obtain probabilities for each class. The class with the highest probability is selected as the predicted label for each input sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6k2vbJBFcU"
   },
   "source": [
    "### Q4.2: In your own words, explain how the forward propagation in your model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZSqrQX-BofT"
   },
   "source": [
    "During forward propagation, the model processes input data by passing it through each layer of the neural network sequentially. For each layer, it computes the weighted sum of the inputs and the layer's weights, adds a bias term if applicable, and then applies an activation function to introduce non-linearity. Specifically, the hidden layers use the sigmoid activation function to capture complex patterns in the data. In the output layer, the model applies the softmax activation function to convert the final linear outputs into probabilities that sum to one across all classes. This process transforms the original input features into a final output that represents the model's confidence in each possible class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDkm7LCOBpOZ"
   },
   "source": [
    "### Q4.3: In your own words, explain how the backward propagation in your model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDgKWePgHHpB"
   },
   "source": [
    "Backward propagation in the model involves computing gradients of the loss function with respect to the network's weights and biases to update them in a way that minimizes the loss. It begins by calculating the error at the output layer, which, due to the use of softmax activation and cross-entropy loss, simplifies to the difference between the predicted probabilities and the one-hot encoded true labels. This error (delta) is then propagated backward through the network by applying the chain rule, multiplying it by the derivative of the activation function of each preceding layer to determine how changes in weights affect the loss. At each layer, the model computes the gradients of the weights by taking the dot product of the transposed activations from the previous layer and the delta, and updates the biases directly using the delta. These gradients are accumulated over all samples, and the model updates the weights and biases by subtracting a fraction of the gradients (scaled by the learning rate and adjusted for regularization) to reduce the overall loss in subsequent iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4v_t-JdB726"
   },
   "source": [
    "### Q4.4: In theory, how do the total number of epochs, the learning rate, and the regularization parameter impact the performance of model? Does any of the theoretical impact actually happen in your result? If so, point them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zFnMPEJG53C"
   },
   "source": [
    "**[Answer]**\n",
    "\n",
    "**Epochs** \n",
    "The total number of epochs determines how many times the model iterates over the entire training dataset; more epochs allow the model to learn more from the data but can lead to overfitting if the model fits the training data too closely, and the predictions do not generalize well to unseen data.\n",
    "\n",
    "**Learning Rate**\n",
    "The learning rate controls the size of the weight updates during training; a higher learning rate can speed up convergence but may cause the model to overshoot minima in the loss function, while a lower learning rate ensures more precise updates but can make training slow and potentially get stuck in local minima.\n",
    "\n",
    "**Regularization Parameter**\n",
    "The regularization parameter adds a penalty for larger weights in the loss function to prevent overfitting; increasing this parameter encourages the model to keep weights small, promoting simplicity and improving generalization to new data, but setting it too high can lead to underfitting by overly restricting the model's capacity to learn from the data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
